import numpy as np
import matplotlib.pyplot as plt
from matplotlib import cm

#%% implement as lamda functions

f = lambda x: (x[0] + x[1]) ** 2 + (np.sin(x[1]) * 3)
f_derivative = lambda x: np.array([(x[0] + x[1]) * 2, (x[0] + x[1]) * 2 + 3 * np.cos(x[1])])

#%%
#x0 = startvalue, A = spd-Matrix, a0 = Startingvalue

# see Alghorhithem 2.42 from lecture
def conjugateGradient(A, b, x0):   
    # initialise step
    n = np.shape(b)[0]
    xk = x0
    dk = rk = b - A*xk
    
    res = []
    res.append(xk)
    
    for k in range(n-1):
        # calculate step
        ak = np.inner(rk, dk) / np.inner(dk, A@dk)
        
        xk = xk + ak*dk
        res.append(xk)
        
        # prepare for next step
        if k < n-1:
            rk_plus1 = rk - ak* A@dk
            
            bk_plus1 = np.inner(rk_plus1, rk_plus1) / np.inner(rk, rk)
            
            dk = rk_plus1 + bk_plus1 * dk
            rk = rk_plus1
    
    return res

def conjugateGradientNL(x0, a0, o, e):
    # initialise step
    n = np.shape(x0)[0]
    xk = x0
    ak = a0
    dk = - f_derivative(xk)
    
    res = []
    res.append(np.hstack((xk,f(xk))))
    
    for k in range(n-1):
        # calculate step
        xk_plus1 = xk + ak * dk
        
        while np.linalg.norm(f(xk_plus1) - f(xk)) > e:
            ak = o * ak
            xk_plus1 = xk + ak * dk
        
        # prepare for next step
        if k < n-1:
            df_xkplus1 = f_derivative(xk_plus1)
            df_xk = f_derivative(xk)
            
            bk_plus1 = np.inner(df_xkplus1, df_xkplus1) / np.inner(df_xk, df_xk)
            dk = - df_xkplus1 + bk_plus1 * dk
            
        xk = xk_plus1
        res.append(np.hstack((xk,f(xk))))
        
        return np.array(res)
        
#%% Test for conjugateGradient
A = np.array([[3, 2], [2, 6]])
b = np.array([2, -8])
x0 = np.array([-2, 2])

print(conjugateGradient(A, b, x0))


#%% visualisation: 
plt.close('all')

# construct x- & y-Values
x_values = np.linspace(-10, 10, 100)
y_values = np.linspace(-10, 10, 100)
x_grid, y_grid = np.meshgrid(x_values, y_values)

# Define the starting points and functions
x0 = np.array([-5, -5])
a0 = 1;
o = 0.5
e = 1e-2

     
# Calculate z according to x- & y-Values
z_grid = np.array([[f(np.array([x, y])) for x in x_values] for y in y_values])

# Draw 3D-Surface
fig = plt.figure()
ax1 = fig.add_subplot(111, projection='3d')
surf = ax1.plot_surface(x_grid, y_grid, z_grid, cmap=cm.YlOrRd, alpha=0.8)

fig.colorbar(surf, shrink=0.5, aspect=5)
        
#BFGS
cgNL_points = conjugateGradientNL(x0, a0, o, e)
drc = np.diff(cgNL_points, axis=0)
ax1.quiver(cgNL_points[:-1,0], cgNL_points[:-1,1], cgNL_points[:-1,2], drc[:,0], drc[:,1], drc[:,2], length = 3, color='blue', label='cgNL')          
                
# Labels
plt.title(f'Figure conjugateGradientNL with point x0 = {x0} in 3D' )
ax1.set_xlabel('X-Achse')
ax1.set_ylabel('Y-Achse')
ax1.set_zlabel('Z-Achse')
        
ax1.legend()
        
plt.show()

def it_steps(cgNL_points):
    return cgNL_points.T[0].size - 1
print('Total number of iterations to reach |xk - x*| < ' + str(e) + ' is: ' + str(it_steps(cgNL_points)))
print('x* = ' + str(cgNL_points[-1]))

#%%
"""
    TODO:
        - Use Armijo-Goldstein instead of o for conjugateGradientNL
        - Fis visualisation
        - Implement the  print(conjugateGradient(A, b, x0)) better
        """
 
